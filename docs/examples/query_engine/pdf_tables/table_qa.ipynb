{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d7688a7",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/output_parsing/table_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c973e-916d-4c9e-9365-e2d5306d7e3d",
   "metadata": {},
   "source": [
    "# Tables QA program"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81e5dde0",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2833cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_TOKEN = \"sk-\"  # Your OpenAI API token here\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms import MockLLM\n",
    "from llama_index.node_parser import (\n",
    "    MarkdownElementNodeParser,\n",
    ")\n",
    "from llama_index.schema import Document, IndexNode, TextNode\n",
    "\n",
    "\n",
    "test_table_document = Document(\n",
    "    text=\"\"\"\n",
    "|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math MMLU|BBH|AGI Eval|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|MPT|7B|20.5|57.4|41.0|57.5|4.9|26.8|31.0|\n",
    "|MPT|30B|28.9|64.9|50.0|64.7|9.1|46.9|38.0|\n",
    "|Falcon|7B|5.6|56.1|42.8|36.0|4.6|26.2|28.0|\n",
    "|Falcon|40B|15.2|69.2|56.7|65.7|12.6|55.4|37.1|\n",
    "|Falcon|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|\n",
    "|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|\n",
    "|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|\n",
    "|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|\n",
    "|Llama 1|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|\n",
    "|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|\n",
    "|Llama 2|34B|27.8|69.9|58.7|68.0|24.2|62.6|44.1|\n",
    "|Llama 2|70B|37.5|71.9|63.6|69.4|35.2|68.9|51.2|\n",
    "\n",
    "    Table 3: Overall performance on grouped academic benchmarks compared to open-source base models.\n",
    "\n",
    "    ‚Ä¢ Popular Aggregated Benchmarks. We report the overall results for MMLU (5 shot) (Hendrycks et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3‚Äì5 shot) (Zhong et al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\n",
    "\n",
    "    As shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the results on MMLU and BBH by ‚á°5 and ‚á°8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B models outperform MPT models of the corresponding size on all categories besides code benchmarks. For the Falcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks. Additionally, Llama 2 70B model outperforms all open-source models.\n",
    "\n",
    "    In addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al., 2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L.\n",
    "\n",
    "    We also analysed the potential data contamination and share the details in Section A.6.\n",
    "\n",
    "|Benchmark (shots)|GPT-3.5|GPT-4|PaLM|PaLM-2-L|Llama 2|\n",
    "|---|---|---|---|---|---|\n",
    "|MMLU (5-shot)|70.0|86.4|69.3|78.3|68.9|\n",
    "|TriviaQA (1-shot)|‚Äì|‚Äì|81.4|86.1|85.0|\n",
    "|Natural Questions (1-shot)|‚Äì|‚Äì|29.3|37.5|33.0|\n",
    "|GSM8K (8-shot)|57.1|92.0|56.5|80.7|56.8|\n",
    "|HumanEval (0-shot)|48.1|67.0|26.2|‚Äì|29.9|\n",
    "|BIG-Bench Hard (3-shot)|‚Äì|‚Äì|52.3|65.7|51.2|\n",
    "\n",
    "    Table 4: Comparison to closed-source models on academic benchmarks. Results for GPT-3.5 and GPT-4 are from OpenAI (2023). Results for the PaLM model are from Chowdhery et al. (2022). Results for the PaLM-2-L are from Anil et al. (2023).\n",
    "\n",
    "    3 Fine-tuning\n",
    "\n",
    "    Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources. In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns (Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.\n",
    "\n",
    "    \n",
    "    We initialize our reward models from pretrained chat model checkpoints, as it ensures that both models\n",
    "    benefit from knowledge acquired in pretraining. In short, the reward model ‚Äúknows‚Äù what the chat model\n",
    "    ---\n",
    "    # Statistics of human preference data for reward modeling\n",
    "\n",
    "| Dataset              | Num. of Comparisons | Avg. # Turns | Avg. # Tokens per Dialogue | Avg. # Tokens per Example | Avg. # Tokens in Prompt | Avg. # Tokens in Response |\n",
    "|----------------------|----------------------|---------------|-----------------------------|---------------------------|-------------------------|---------------------------|\n",
    "| Anthropic Helpful    | 122,387              | 3.0           | 251.5                       | 17.7                      | 88.4                    |\n",
    "| Anthropic Harmless   | 43,966               | 3.0           | 152.5                       | 15.7                      | 46.4                    |\n",
    "| OpenAI Summarize     | 176,625              | 1.0           | 371.1                       | 336.0                     | 35.1                    |\n",
    "| OpenAI WebGPT        | 13,333               | 1.0           | 237.2                       | 48.3                      | 188.9                   |\n",
    "| StackExchange        | 1,038,480            | 1.0           | 440.2                       | 200.1                     | 240.2                   |\n",
    "| Stanford SHP         | 74,882               | 1.0           | 338.3                       | 199.5                     | 138.8                   |\n",
    "| Synthetic GPT-J      | 33,139               | 1.0           | 123.3                       | 13.0                      | 110.3                   |\n",
    "| Meta (Safety & Helpfulness) | 1,418,091     | 3.9           | 798.5                       | 31.4                      | 234.1                   |\n",
    "| Total                | 2,919,326            | 1.6           | 595.7                       | 108.2                     | 216.9                   |\n",
    "\n",
    "    Table 6: Statistics of human preference data for reward modeling. We list both the open-source and internally collected human preference data used for reward modeling. Note that a binary human preference comparison contains 2 responses (chosen and rejected) sharing the same prompt (and previous dialogue). Each example consists of a prompt (including previous dialogue if available) and a response, which is the input of the reward model. We report the number of comparisons, the average number of turns per dialogue, the average number of tokens per example, per prompt and per response. More details on Meta helpfulness and safety data per batch can be found in Appendix A.3.1.\n",
    "\n",
    "    Training Objectives  \n",
    "    To train the reward model, we convert our collected pairwise human preference data into a binary ranking label format (i.e., chosen & rejected) and enforce the chosen response to have a higher score than its counterpart. We used a binary ranking loss consistent with Ouyang et al. (2022):\n",
    "\n",
    "        \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33aa3a6",
   "metadata": {},
   "source": [
    "## Baseline 1: Using MarkDown Raw Output (containing multipel tables) as Input for Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63705eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of MPT 30B for common sense reasoning is 64.9.\n",
      "PaLM-2-L performance for TriviaQA is 86.1.\n",
      "Llama 2's performance for HumanEval is 29.9.\n",
      "Llama 2's performance for AGI Eval is not mentioned in the given context.\n",
      "LLAMA 2's performance for HumanEval and AGI Eval is not mentioned in the given context.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents([test_table_document])\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "# query different questions\n",
    "response_1 = query_engine.query(\n",
    "    \"What is MPT 30b performance for common sense reasoning?\"\n",
    ")\n",
    "print(response_1)\n",
    "\n",
    "response_2 = query_engine.query(\"What is PaLM-2-L performance for TriviaQA?\")\n",
    "print(response_2)\n",
    "\n",
    "\n",
    "response_3 = query_engine.query(\"What is LLAMA 2 performance for HumanEval?\")\n",
    "print(response_3)\n",
    "\n",
    "response_4 = query_engine.query(\"What is LLAMA 2 performance for AGI Eval?\")\n",
    "print(response_4)\n",
    "\n",
    "response_5 = query_engine.query(\n",
    "    \"What is LLAMA 2 performance for HumanEval and AGI Eval?\"\n",
    ")\n",
    "print(response_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b37fe",
   "metadata": {},
   "source": [
    "## Baseline 2: Apply `MarkdownElementNodeParser` for parsing table nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14403f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:15,  7.67s/it]\n"
     ]
    }
   ],
   "source": [
    "node_parser = MarkdownElementNodeParser()\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents([test_table_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ee6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824811c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Base Node]---\n",
      "This table compares different models based on various metrics such as commonsense reasoning, world knowledge, reading comprehension, math MMLU, BBH, and AGI evaluation. It also compares benchmark performance of GPT-3.5, GPT-4, PaLM, PaLM-2-L, and Llama 2 on different tasks such as MMLU, TriviaQA, Natural Questions, GSM8K, HumanEval, and BIG-Bench Hard.,\n",
      "with the following table title:\n",
      "Comparison of Models and Benchmark Performance,\n",
      "with the following columns:\n",
      "- Model: The name of the model\n",
      "- Size: The size of the model\n",
      "- Code: The code metric\n",
      "- Commonsense Reasoning: The commonsense reasoning metric\n",
      "- World Knowledge: The world knowledge metric\n",
      "- Reading Comprehension: The reading comprehension metric\n",
      "- Math MMLU: The math MMLU metric\n",
      "- BBH: The BBH metric\n",
      "- AGI Eval: The AGI evaluation metric\n",
      "\n",
      "[Base Node]---\n",
      "Table 3: Overall performance on grouped academic benchmarks compared to open-source base models.\n",
      "\n",
      "    ‚Ä¢ Popular Aggregated Benchmarks. We report the overall results for MMLU (5 shot) (Hendrycks et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3‚Äì5 shot) (Zhong et al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\n",
      "\n",
      "    As shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the results on MMLU and BBH by ‚á°5 and ‚á°8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B models outperform MPT models of the corresponding size on all categories besides code benchmarks. For the Falcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks. Additionally, Llama 2 70B model outperforms all open-source models.\n",
      "\n",
      "    In addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al., 2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L.\n",
      "\n",
      "    We also analysed the potential data contamination and share the details in Section A.6.\n",
      "[Base Node]---\n",
      "This table compares different models based on their performance in various benchmarks.,\n",
      "with the following table title:\n",
      "Comparison of Model Performance in Benchmarks,\n",
      "with the following columns:\n",
      "- Benchmark (shots): The name of the benchmark and the number of shots used.\n",
      "- GPT-3.5: Performance of the GPT-3.5 model in the benchmark.\n",
      "- GPT-4: Performance of the GPT-4 model in the benchmark.\n",
      "- PaLM: Performance of the PaLM model in the benchmark.\n",
      "- PaLM-2-L: Performance of the PaLM-2-L model in the benchmark.\n",
      "- Llama 2: Performance of the Llama 2 model in the benchmark.\n",
      "\n",
      "[Base Node]---\n",
      "Table 4: Comparison to closed-source models on academic benchmarks. Results for GPT-3.5 and GPT-4 are from OpenAI (2023). Results for the PaLM model are from Chowdhery et al. (2022). Results for the PaLM-2-L are from Anil et al. (2023).\n",
      "\n",
      "    3 Fine-tuning\n",
      "\n",
      "    Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources. In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns (Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.\n",
      "\n",
      "    \n",
      "We initialize our reward models from pretrained chat model checkpoints, as it ensures that both models\n",
      "benefit from knowledge acquired in pretraining. In short, the reward model ‚Äúknows‚Äù what the chat model\n",
      "---\n",
      " Statistics of human preference data for reward modeling\n",
      "\n",
      "| Dataset              | Num. of Comparisons | Avg. # Turns | Avg. # Tokens per Dialogue | Avg. # Tokens per Example | Avg. # Tokens in Prompt | Avg. # Tokens in Response |\n",
      "|----------------------|----------------------|---------------|-----------------------------|---------------------------|-------------------------|---------------------------|\n",
      "| Anthropic Helpful    | 122,387              | 3.0           | 251.5                       | 17.7                      | 88.4                    |\n",
      "| Anthropic Harmless   | 43,966               | 3.0           | 152.5                       | 15.7                      | 46.4                    |\n",
      "| OpenAI Summarize     | 176,625              | 1.0           | 371.1                       | 336.0                     | 35.1                    |\n",
      "| OpenAI WebGPT        | 13,333               | 1.0           | 237.2                       | 48.3                      | 188.9                   |\n",
      "| StackExchange        | 1,038,480            | 1.0           | 440.2                       | 200.1                     | 240.2                   |\n",
      "| Stanford SHP         | 74,882               | 1.0           | 338.3                       | 199.5                     | 138.8                   |\n",
      "| Synthetic GPT-J      | 33,139               | 1.0           | 123.3                       | 13.0                      | 110.3                   |\n",
      "| Meta (Safety & Helpfulness) | 1,418,091     | 3.9           | 798.5                       | 31.4                      | 234.1                   |\n",
      "| Total                | 2,919,326            | 1.6           | 595.7                       | 108.2                     | 216.9                   |\n",
      "\n",
      "Table 6: Statistics of human preference data for reward modeling. We list both the open-source and internally collected human preference data used for reward modeling. Note that a binary human preference comparison contains 2 responses (chosen and rejected) sharing the same prompt (and previous dialogue). Each example consists of a prompt (including previous dialogue if available) and a response, which is the input of the reward model. We report the number of comparisons, the average number of turns per dialogue, the average number of tokens per example, per prompt and per response. More details on Meta helpfulness and safety data per batch can be found in Appendix A.3.1.\n",
      "\n",
      "Training Objectives  \n",
      "To train the reward model, we convert our collected pairwise human preference data into a binary ranking label format (i.e., chosen & rejected) and enforce the chosen response to have a higher score than its counterpart. We used a binary ranking loss consistent with Ouyang et al. (2022):\n"
     ]
    }
   ],
   "source": [
    "base_nodes, node_mappings = node_parser.get_base_nodes_and_mappings(nodes)\n",
    "\n",
    "# for node in node_mappings:\n",
    "#     print(\"[Node maping]-----------------\")\n",
    "\n",
    "# print(node)\n",
    "# print(node_mappings[node])\n",
    "# print(node.metadata)\n",
    "\n",
    "for node in base_nodes:\n",
    "    print(\"[Base Node]---\")\n",
    "    # for d in node.metadata:\n",
    "    #     print(d)\n",
    "    # print(node.metadata)\n",
    "    print(node.text)\n",
    "\n",
    "# for node in nodes:\n",
    "#     print(\"[node]---\")\n",
    "#     print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae94e55",
   "metadata": {},
   "source": [
    "## Baseline 2: Table Retrieval and QA using recursive retrieval and query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import RecursiveRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.embeddings import TogetherEmbedding, OpenAIEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.service_context import ServiceContext\n",
    "\n",
    "# construct top-level vector index + query engine\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=3)\n",
    "vector_query_engine = vector_index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "from llama_index.retrievers import RecursiveRetriever\n",
    "\n",
    "recursive_retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever},\n",
    "    node_dict=node_mappings,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "recursive_query_engine = RetrieverQueryEngine.from_args(\n",
    "    recursive_retriever, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbf239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: What is MPT 30b performance for common sense reasoning?\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Table 20: Performance on standard benchmarks.\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: This table provides information on different models and their performance in various benchmarks.,\n",
      "with the following table title:\n",
      "Model Performance in Benchmarks,\n",
      "with the following columns:\n",
      "- Model: The name of the model\n",
      "- Size: The size of the model\n",
      "- Code: The code performance of the model\n",
      "- Commonsense Reasoning: The performance of the model in commonsense reasoning tasks\n",
      "- World Knowledge: The performance of the model in world knowledge tasks\n",
      "- Reading Comprehension: The performance of the model in reading comprehension tasks\n",
      "- Math MMLU: The performance of the model in math MMLU tasks\n",
      "- BBH: The performance of the model in BBH tasks\n",
      "- AGI Eval: The performance of the model in AGI evaluation\n",
      "\n",
      "|Benchmark (shots)|GPT-3.5|GPT-4|PaLM|PaLM-2-L|Llama 2|\n",
      "|---|---|---|---|---|---|\n",
      "|MMLU (5-shot)|70.0|86.4|69.3|78.3|68.9|\n",
      "|TriviaQA (1-shot)|‚Äì|‚Äì|81.4|86.1|85.0|\n",
      "|Natural Questions (1-shot)|‚Äì|‚Äì|29.3|37.5|33.0|\n",
      "|GSM8K (8-shot)|57.1|92.0|56.5|80.7|56.8|\n",
      "|HumanEval (0-shot)|48.1|67.0|26.2|‚Äì|29.9|\n",
      "|BIG-Bench Hard (3-shot)|‚Äì|‚Äì|52.3|65.7|51.2|\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: This table compares different models based on various metrics such as commonsense reasoning, world knowledge, reading comprehension, math MMLU, BBH, and AGI evaluation. It also includes benchmark scores for different models on tasks like MMLU, TriviaQA, Natural Questions, GSM8K, HumanEval, and BIG-Bench Hard.,\n",
      "with the following table title:\n",
      "Comparison of Models and Benchmark Scores,\n",
      "with the following columns:\n",
      "- Model: The name of the model\n",
      "- Size: The size of the model\n",
      "- Code: The code metric\n",
      "- Commonsense Reasoning: The score for commonsense reasoning\n",
      "- World Knowledge: The score for world knowledge\n",
      "- Reading Comprehension: The score for reading comprehension\n",
      "- Math MMLU: The score for math MMLU\n",
      "- BBH: The score for BBH\n",
      "- AGI Eval: The score for AGI evaluation\n",
      "\n",
      "|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math MMLU|BBH|AGI Eval|\n",
      "|---|---|---|---|---|---|---|---|---|\n",
      "|MPT|7B|20.5|57.4|41.0|57.5|4.9|26.8|31.0|\n",
      "|MPT|30B|28.9|64.9|50.0|64.7|9.1|46.9|38.0|\n",
      "|Falcon|7B|5.6|56.1|42.8|36.0|4.6|26.2|28.0|\n",
      "|Falcon|40B|15.2|69.2|56.7|65.7|12.6|55.4|37.1|\n",
      "|Falcon|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|\n",
      "|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|\n",
      "|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|\n",
      "|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|\n",
      "|Llama 1|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|\n",
      "|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|\n",
      "|Llama 2|34B|27.8|69.9|58.7|68.0|24.2|62.6|44.1|\n",
      "|Llama 2|70B|37.5|71.9|63.6|69.4|35.2|68.9|51.2|\n",
      "\n",
      "\u001b[0mThe performance of the MPT 30B model for commonsense reasoning is 64.9.\n",
      "\u001b[1;3;34mRetrieving with query id None: What is PaLM-2-L performance for TriviaQA?\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Table 4: Comparison to closed-source models on academic benchmarks. Results for GPT-3.5 and GPT-4 are from OpenAI (2023). Results for the PaLM model are from Chowdhery et al. (2022). Results for the PaLM-2-L are from Anil et al. (2023).\n",
      "\n",
      "    3 Fine-tuning\n",
      "\n",
      "    Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources. In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns (Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.\n",
      "\n",
      "    \n",
      "We initialize our reward models from pretrained chat model checkpoints, as it ensures that both models\n",
      "benefit from knowledge acquired in pretraining. In short, the reward model ‚Äúknows‚Äù what the chat model\n",
      "---\n",
      " Statistics of human preference data for reward modeling\n",
      "\n",
      "| Dataset              | Num. of Comparisons | Avg. # Turns | Avg. # Tokens per Dialogue | Avg. # Tokens per Example | Avg. # Tokens in Prompt | Avg. # Tokens in Response |\n",
      "|----------------------|----------------------|---------------|-----------------------------|---------------------------|-------------------------|---------------------------|\n",
      "| Anthropic Helpful    | 122,387              | 3.0           | 251.5                       | 17.7                      | 88.4                    |\n",
      "| Anthropic Harmless   | 43,966               | 3.0           | 152.5                       | 15.7                      | 46.4                    |\n",
      "| OpenAI Summarize     | 176,625              | 1.0           | 371.1                       | 336.0                     | 35.1                    |\n",
      "| OpenAI WebGPT        | 13,333               | 1.0           | 237.2                       | 48.3                      | 188.9                   |\n",
      "| StackExchange        | 1,038,480            | 1.0           | 440.2                       | 200.1                     | 240.2                   |\n",
      "| Stanford SHP         | 74,882               | 1.0           | 338.3                       | 199.5                     | 138.8                   |\n",
      "| Synthetic GPT-J      | 33,139               | 1.0           | 123.3                       | 13.0                      | 110.3                   |\n",
      "| Meta (Safety & Helpfulness) | 1,418,091     | 3.9           | 798.5                       | 31.4                      | 234.1                   |\n",
      "| Total                | 2,919,326            | 1.6           | 595.7                       | 108.2                     | 216.9                   |\n",
      "\n",
      "Table 6: Statistics of human preference data for reward modeling. We list both the open-source and internally collected human preference data used for reward modeling. Note that a binary human preference comparison contains 2 responses (chosen and rejected) sharing the same prompt (and previous dialogue). Each example consists of a prompt (including previous dialogue if available) and a response, which is the input of the reward model. We report the number of comparisons, the average number of turns per dialogue, the average number of tokens per example, per prompt and per response. More details on Meta helpfulness and safety data per batch can be found in Appendix A.3.1.\n",
      "\n",
      "Training Objectives  \n",
      "To train the reward model, we convert our collected pairwise human preference data into a binary ranking label format (i.e., chosen & rejected) and enforce the chosen response to have a higher score than its counterpart. We used a binary ranking loss consistent with Ouyang et al. (2022):\n",
      "\n",
      "A.2.2 Additional Details for Pretrained Models Evaluation\n",
      "\n",
      "MMLU details. In Table 19, we report details of the MMLU (Hendrycks et al., 2020) evaluation for Llama 2 models and others open-source models.\n",
      "\n",
      "Standard Benchmarks. In Table 20, we show results on several standard benchmarks.\n",
      "\n",
      "Code Generation. In Table 21, we compare results of Llama 2 with popular open source models on the Human-Eval and MBPP code generation benchmarks.\n",
      "\n",
      "World Knowledge.\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: This table provides performance metrics for various language models on different question answering tasks.,\n",
      "with the following table title:\n",
      "Performance Metrics for Language Models on Question Answering Tasks,\n",
      "with the following columns:\n",
      "- : Performance on BoolQ task\n",
      "- : Performance on PIQA task\n",
      "- : Performance on SIQA task\n",
      "- : Performance on HellaSwag task\n",
      "- : Performance on WinoGrande task\n",
      "- : Performance on ARC-e task\n",
      "- : Performance on ARC-c task\n",
      "- : Performance on OBQA task\n",
      "- : Performance on CSQA task\n",
      "- : Performance on MMLU task\n",
      "\n",
      "|             | Human-Eval |           | MBPP |           .1|\n",
      "|---|---|---|---|---|\n",
      "|             |            |           |        |           |\n",
      "| MPT         | 7B         | 18.3      | -      | 22.6      |\n",
      "|             | 30B        | 25.0      | -      | 32.8      |\n",
      "| Falcon      | 7B         | 0.0       | -      | 11.2      |\n",
      "|             | 40B        | 0.6       | -      | 29.8      |\n",
      "|             | 7B         | 10.5      | 36.5   | 17.7      |\n",
      "| Llama 1     | 13B        | 15.8      | 52.5   | 22.0      |\n",
      "|             | 33B        | 21.7      | 70.7   | 30.2      |\n",
      "|             | 65B        | 23.7      | 79.3   | 37.7      |\n",
      "|             | 7B         | 12.8      | 45.6   | 20.8      |\n",
      "| Llama 2     | 13B        | 18.3      | 60.2   | 30.6      |\n",
      "|             | 34B        | 22.6      | 77.2   | 33.0      |\n",
      "|             | 70B        | 29.9      | 89.0   | 45.0      |\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: id_11_table\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id id_11_table: What is PaLM-2-L performance for TriviaQA?\n",
      "\u001b[0mThe context does not provide information on the performance of the PaLM-2-L model for TriviaQA.\n",
      "\u001b[1;3;34mRetrieving with query id None: What is LLAMA 2 performance for HumanEval?\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Training Objectives  \n",
      "To train the reward model, we convert our collected pairwise human preference data into a binary ranking label format (i.e., chosen & rejected) and enforce the chosen response to have a higher score than its counterpart. We used a binary ranking loss consistent with Ouyang et al. (2022):\n",
      "\n",
      "A.2.2 Additional Details for Pretrained Models Evaluation\n",
      "\n",
      "MMLU details. In Table 19, we report details of the MMLU (Hendrycks et al., 2020) evaluation for Llama 2 models and others open-source models.\n",
      "\n",
      "Standard Benchmarks. In Table 20, we show results on several standard benchmarks.\n",
      "\n",
      "Code Generation. In Table 21, we compare results of Llama 2 with popular open source models on the Human-Eval and MBPP code generation benchmarks.\n",
      "\n",
      "World Knowledge. We evaluate the Llama 2 model together with other open-source models on the NaturalQuestions and TriviaQA benchmarks (Table 22).\n",
      "\n",
      "Reading Comprehension In Table 23 we report zero-shot and few-shot results on SQUAD and zero-shot and one-shot experiments on QUAC. Here Llama 2 performs best on all evaluation settings and models except the QUAC 0-shot where Llama 1 30B performs slightly better.\n",
      "\n",
      "Exams. In Table 24, we present fine-grained results from the English part of the AGI Eval (Zhong et al., 2023) benchmark. AGI Eval is a collection of standardized exams in different subjects.\n",
      "---\n",
      " Table 19: Five-shot performance on the Massive Multitask Language Understanding (MMLU) benchmark.\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Table 3: Overall performance on grouped academic benchmarks compared to open-source base models.\n",
      "\n",
      "    ‚Ä¢ Popular Aggregated Benchmarks. We report the overall results for MMLU (5 shot) (Hendrycks et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3‚Äì5 shot) (Zhong et al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\n",
      "\n",
      "    As shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the results on MMLU and BBH by ‚á°5 and ‚á°8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B models outperform MPT models of the corresponding size on all categories besides code benchmarks. For the Falcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks. Additionally, Llama 2 70B model outperforms all open-source models.\n",
      "\n",
      "    In addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al., 2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L.\n",
      "\n",
      "    We also analysed the potential data contamination and share the details in Section A.6.\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Table 4: Comparison to closed-source models on academic benchmarks. Results for GPT-3.5 and GPT-4 are from OpenAI (2023). Results for the PaLM model are from Chowdhery et al. (2022). Results for the PaLM-2-L are from Anil et al. (2023).\n",
      "\n",
      "    3 Fine-tuning\n",
      "\n",
      "    Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources. In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns (Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.\n",
      "\n",
      "    \n",
      "We initialize our reward models from pretrained chat model checkpoints, as it ensures that both models\n",
      "benefit from knowledge acquired in pretraining. In short, the reward model ‚Äúknows‚Äù what the chat model\n",
      "---\n",
      " Statistics of human preference data for reward modeling\n",
      "\n",
      "| Dataset              | Num. of Comparisons | Avg. # Turns | Avg. # Tokens per Dialogue | Avg. # Tokens per Example | Avg. # Tokens in Prompt | Avg. # Tokens in Response |\n",
      "|----------------------|----------------------|---------------|-----------------------------|---------------------------|-------------------------|---------------------------|\n",
      "| Anthropic Helpful    | 122,387              | 3.0           | 251.5                       | 17.7                      | 88.4                    |\n",
      "| Anthropic Harmless   | 43,966               | 3.0           | 152.5                       | 15.7                      | 46.4                    |\n",
      "| OpenAI Summarize     | 176,625              | 1.0           | 371.1                       | 336.0                     | 35.1                    |\n",
      "| OpenAI WebGPT        | 13,333               | 1.0           | 237.2                       | 48.3                      | 188.9                   |\n",
      "| StackExchange        | 1,038,480            | 1.0           | 440.2                       | 200.1                     | 240.2                   |\n",
      "| Stanford SHP         | 74,882               | 1.0           | 338.3                       | 199.5                     | 138.8                   |\n",
      "| Synthetic GPT-J      | 33,139               | 1.0           | 123.3                       | 13.0                      | 110.3                   |\n",
      "| Meta (Safety & Helpfulness) | 1,418,091     | 3.9           | 798.5                       | 31.4                      | 234.1                   |\n",
      "| Total                | 2,919,326            | 1.6           | 595.7                       | 108.2                     | 216.9                   |\n",
      "\n",
      "Table 6: Statistics of human preference data for reward modeling. We list both the open-source and internally collected human preference data used for reward modeling. Note that a binary human preference comparison contains 2 responses (chosen and rejected) sharing the same prompt (and previous dialogue). Each example consists of a prompt (including previous dialogue if available) and a response, which is the input of the reward model. We report the number of comparisons, the average number of turns per dialogue, the average number of tokens per example, per prompt and per response. More details on Meta helpfulness and safety data per batch can be found in Appendix A.3.1.\n",
      "\n",
      "Training Objectives  \n",
      "To train the reward model, we convert our collected pairwise human preference data into a binary ranking label format (i.e., chosen & rejected) and enforce the chosen response to have a higher score than its counterpart. We used a binary ranking loss consistent with Ouyang et al. (2022):\n",
      "\n",
      "A.2.2 Additional Details for Pretrained Models Evaluation\n",
      "\n",
      "MMLU details. In Table 19, we report details of the MMLU (Hendrycks et al., 2020) evaluation for Llama 2 models and others open-source models.\n",
      "\n",
      "Standard Benchmarks. In Table 20, we show results on several standard benchmarks.\n",
      "\n",
      "Code Generation. In Table 21, we compare results of Llama 2 with popular open source models on the Human-Eval and MBPP code generation benchmarks.\n",
      "\n",
      "World Knowledge.\n",
      "\u001b[0mThe context does not provide specific information on Llama 2's performance on the HumanEval benchmark.\n",
      "\u001b[1;3;34mRetrieving with query id None: What is LLAMA 2 performance for AGI Eval?\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Table 3: Overall performance on grouped academic benchmarks compared to open-source base models.\n",
      "\n",
      "    ‚Ä¢ Popular Aggregated Benchmarks. We report the overall results for MMLU (5 shot) (Hendrycks et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3‚Äì5 shot) (Zhong et al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\n",
      "\n",
      "    As shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the results on MMLU and BBH by ‚á°5 and ‚á°8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B models outperform MPT models of the corresponding size on all categories besides code benchmarks. For the Falcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks. Additionally, Llama 2 70B model outperforms all open-source models.\n",
      "\n",
      "    In addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al., 2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L.\n",
      "\n",
      "    We also analysed the potential data contamination and share the details in Section A.6.\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Training Objectives  \n",
      "To train the reward model, we convert our collected pairwise human preference data into a binary ranking label format (i.e., chosen & rejected) and enforce the chosen response to have a higher score than its counterpart. We used a binary ranking loss consistent with Ouyang et al. (2022):\n",
      "\n",
      "A.2.2 Additional Details for Pretrained Models Evaluation\n",
      "\n",
      "MMLU details. In Table 19, we report details of the MMLU (Hendrycks et al., 2020) evaluation for Llama 2 models and others open-source models.\n",
      "\n",
      "Standard Benchmarks. In Table 20, we show results on several standard benchmarks.\n",
      "\n",
      "Code Generation. In Table 21, we compare results of Llama 2 with popular open source models on the Human-Eval and MBPP code generation benchmarks.\n",
      "\n",
      "World Knowledge. We evaluate the Llama 2 model together with other open-source models on the NaturalQuestions and TriviaQA benchmarks (Table 22).\n",
      "\n",
      "Reading Comprehension In Table 23 we report zero-shot and few-shot results on SQUAD and zero-shot and one-shot experiments on QUAC. Here Llama 2 performs best on all evaluation settings and models except the QUAC 0-shot where Llama 1 30B performs slightly better.\n",
      "\n",
      "Exams. In Table 24, we present fine-grained results from the English part of the AGI Eval (Zhong et al., 2023) benchmark. AGI Eval is a collection of standardized exams in different subjects.\n",
      "---\n",
      " Table 19: Five-shot performance on the Massive Multitask Language Understanding (MMLU) benchmark.\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: This table compares different models based on various metrics such as commonsense reasoning, world knowledge, reading comprehension, math MMLU, BBH, and AGI evaluation. It also includes benchmark scores for different models on tasks like MMLU, TriviaQA, Natural Questions, GSM8K, HumanEval, and BIG-Bench Hard.,\n",
      "with the following table title:\n",
      "Comparison of Models and Benchmark Scores,\n",
      "with the following columns:\n",
      "- Model: The name of the model\n",
      "- Size: The size of the model\n",
      "- Code: The code metric\n",
      "- Commonsense Reasoning: The score for commonsense reasoning\n",
      "- World Knowledge: The score for world knowledge\n",
      "- Reading Comprehension: The score for reading comprehension\n",
      "- Math MMLU: The score for math MMLU\n",
      "- BBH: The score for BBH\n",
      "- AGI Eval: The score for AGI evaluation\n",
      "\n",
      "|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math MMLU|BBH|AGI Eval|\n",
      "|---|---|---|---|---|---|---|---|---|\n",
      "|MPT|7B|20.5|57.4|41.0|57.5|4.9|26.8|31.0|\n",
      "|MPT|30B|28.9|64.9|50.0|64.7|9.1|46.9|38.0|\n",
      "|Falcon|7B|5.6|56.1|42.8|36.0|4.6|26.2|28.0|\n",
      "|Falcon|40B|15.2|69.2|56.7|65.7|12.6|55.4|37.1|\n",
      "|Falcon|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|\n",
      "|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|\n",
      "|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|\n",
      "|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|\n",
      "|Llama 1|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|\n",
      "|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|\n",
      "|Llama 2|34B|27.8|69.9|58.7|68.0|24.2|62.6|44.1|\n",
      "|Llama 2|70B|37.5|71.9|63.6|69.4|35.2|68.9|51.2|\n",
      "\n",
      "\u001b[0mThe performance of the Llama 2 model for AGI Eval varies depending on the size of the model. For the 13B model, the score is 39.4. For the 34B model, the score is 44.1. The 70B model has the highest score at 51.2.\n",
      "\u001b[1;3;34mRetrieving with query id None: What is LLAMA 2 performance for HumanEval and AGI Eval?\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Training Objectives  \n",
      "To train the reward model, we convert our collected pairwise human preference data into a binary ranking label format (i.e., chosen & rejected) and enforce the chosen response to have a higher score than its counterpart. We used a binary ranking loss consistent with Ouyang et al. (2022):\n",
      "\n",
      "A.2.2 Additional Details for Pretrained Models Evaluation\n",
      "\n",
      "MMLU details. In Table 19, we report details of the MMLU (Hendrycks et al., 2020) evaluation for Llama 2 models and others open-source models.\n",
      "\n",
      "Standard Benchmarks. In Table 20, we show results on several standard benchmarks.\n",
      "\n",
      "Code Generation. In Table 21, we compare results of Llama 2 with popular open source models on the Human-Eval and MBPP code generation benchmarks.\n",
      "\n",
      "World Knowledge. We evaluate the Llama 2 model together with other open-source models on the NaturalQuestions and TriviaQA benchmarks (Table 22).\n",
      "\n",
      "Reading Comprehension In Table 23 we report zero-shot and few-shot results on SQUAD and zero-shot and one-shot experiments on QUAC. Here Llama 2 performs best on all evaluation settings and models except the QUAC 0-shot where Llama 1 30B performs slightly better.\n",
      "\n",
      "Exams. In Table 24, we present fine-grained results from the English part of the AGI Eval (Zhong et al., 2023) benchmark. AGI Eval is a collection of standardized exams in different subjects.\n",
      "---\n",
      " Table 19: Five-shot performance on the Massive Multitask Language Understanding (MMLU) benchmark.\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Table 3: Overall performance on grouped academic benchmarks compared to open-source base models.\n",
      "\n",
      "    ‚Ä¢ Popular Aggregated Benchmarks. We report the overall results for MMLU (5 shot) (Hendrycks et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3‚Äì5 shot) (Zhong et al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\n",
      "\n",
      "    As shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the results on MMLU and BBH by ‚á°5 and ‚á°8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B models outperform MPT models of the corresponding size on all categories besides code benchmarks. For the Falcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks. Additionally, Llama 2 70B model outperforms all open-source models.\n",
      "\n",
      "    In addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al., 2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L.\n",
      "\n",
      "    We also analysed the potential data contamination and share the details in Section A.6.\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Table 4: Comparison to closed-source models on academic benchmarks. Results for GPT-3.5 and GPT-4 are from OpenAI (2023). Results for the PaLM model are from Chowdhery et al. (2022). Results for the PaLM-2-L are from Anil et al. (2023).\n",
      "\n",
      "    3 Fine-tuning\n",
      "\n",
      "    Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources. In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns (Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.\n",
      "\n",
      "    \n",
      "We initialize our reward models from pretrained chat model checkpoints, as it ensures that both models\n",
      "benefit from knowledge acquired in pretraining. In short, the reward model ‚Äúknows‚Äù what the chat model\n",
      "---\n",
      " Statistics of human preference data for reward modeling\n",
      "\n",
      "| Dataset              | Num. of Comparisons | Avg. # Turns | Avg. # Tokens per Dialogue | Avg. # Tokens per Example | Avg. # Tokens in Prompt | Avg. # Tokens in Response |\n",
      "|----------------------|----------------------|---------------|-----------------------------|---------------------------|-------------------------|---------------------------|\n",
      "| Anthropic Helpful    | 122,387              | 3.0           | 251.5                       | 17.7                      | 88.4                    |\n",
      "| Anthropic Harmless   | 43,966               | 3.0           | 152.5                       | 15.7                      | 46.4                    |\n",
      "| OpenAI Summarize     | 176,625              | 1.0           | 371.1                       | 336.0                     | 35.1                    |\n",
      "| OpenAI WebGPT        | 13,333               | 1.0           | 237.2                       | 48.3                      | 188.9                   |\n",
      "| StackExchange        | 1,038,480            | 1.0           | 440.2                       | 200.1                     | 240.2                   |\n",
      "| Stanford SHP         | 74,882               | 1.0           | 338.3                       | 199.5                     | 138.8                   |\n",
      "| Synthetic GPT-J      | 33,139               | 1.0           | 123.3                       | 13.0                      | 110.3                   |\n",
      "| Meta (Safety & Helpfulness) | 1,418,091     | 3.9           | 798.5                       | 31.4                      | 234.1                   |\n",
      "| Total                | 2,919,326            | 1.6           | 595.7                       | 108.2                     | 216.9                   |\n",
      "\n",
      "Table 6: Statistics of human preference data for reward modeling. We list both the open-source and internally collected human preference data used for reward modeling. Note that a binary human preference comparison contains 2 responses (chosen and rejected) sharing the same prompt (and previous dialogue). Each example consists of a prompt (including previous dialogue if available) and a response, which is the input of the reward model. We report the number of comparisons, the average number of turns per dialogue, the average number of tokens per example, per prompt and per response. More details on Meta helpfulness and safety data per batch can be found in Appendix A.3.1.\n",
      "\n",
      "Training Objectives  \n",
      "To train the reward model, we convert our collected pairwise human preference data into a binary ranking label format (i.e., chosen & rejected) and enforce the chosen response to have a higher score than its counterpart. We used a binary ranking loss consistent with Ouyang et al. (2022):\n",
      "\n",
      "A.2.2 Additional Details for Pretrained Models Evaluation\n",
      "\n",
      "MMLU details. In Table 19, we report details of the MMLU (Hendrycks et al., 2020) evaluation for Llama 2 models and others open-source models.\n",
      "\n",
      "Standard Benchmarks. In Table 20, we show results on several standard benchmarks.\n",
      "\n",
      "Code Generation. In Table 21, we compare results of Llama 2 with popular open source models on the Human-Eval and MBPP code generation benchmarks.\n",
      "\n",
      "World Knowledge.\n",
      "\u001b[0mThe exact performance metrics for Llama 2 on HumanEval and AGI Eval are not provided in the context. However, it is mentioned that Llama 2 models generally outperform Llama 1 models and other open-source models on various benchmarks. For AGI Eval, only the English tasks were evaluated and the average results were reported. On the other hand, there is a significant gap in performance between Llama 2 and some closed-source models on coding benchmarks, which might include HumanEval.\n"
     ]
    }
   ],
   "source": [
    "# query different questions\n",
    "response_1 = recursive_query_engine.query(\n",
    "    \"What is MPT 30b performance for common sense reasoning?\"\n",
    ")\n",
    "print(response_1)\n",
    "\n",
    "response_2 = recursive_query_engine.query(\n",
    "    \"What is PaLM-2-L performance for TriviaQA?\"\n",
    ")\n",
    "print(response_2)\n",
    "\n",
    "\n",
    "response_3 = recursive_query_engine.query(\n",
    "    \"What is LLAMA 2 performance for HumanEval?\"\n",
    ")\n",
    "print(response_3)\n",
    "\n",
    "response_4 = recursive_query_engine.query(\n",
    "    \"What is LLAMA 2 performance for AGI Eval?\"\n",
    ")\n",
    "print(response_4)\n",
    "\n",
    "response_5 = recursive_query_engine.query(\n",
    "    \"What is LLAMA 2 performance for HumanEval and AGI Eval?\"\n",
    ")\n",
    "print(response_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d48d604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the MPT 30B model for commonsense reasoning is 64.9.\n",
      "The context does not provide information on the performance of the PaLM-2-L model for TriviaQA.\n",
      "The context does not provide specific information on Llama 2's performance on the HumanEval benchmark.\n",
      "The performance of the Llama 2 model for AGI Eval varies depending on the size of the model. For the 13B model, the score is 39.4. For the 34B model, the score is 44.1. The 70B model has the highest score at 51.2.\n",
      "The exact performance metrics for Llama 2 on HumanEval and AGI Eval are not provided in the context. However, it is mentioned that Llama 2 models generally outperform Llama 1 models and other open-source models on various benchmarks. For AGI Eval, only the English tasks were evaluated and the average results were reported. On the other hand, there is a significant gap in performance between Llama 2 and some closed-source models on coding benchmarks, which might include HumanEval.\n"
     ]
    }
   ],
   "source": [
    "print(response_1)\n",
    "print(response_2)\n",
    "print(response_3)\n",
    "print(response_4)\n",
    "print(response_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f3af3",
   "metadata": {},
   "source": [
    "## Baseline 3 Text-to-pandas query engine for Table Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Size</th>\n",
       "      <th>Code</th>\n",
       "      <th>Commonsense Reasoning</th>\n",
       "      <th>World Knowledge</th>\n",
       "      <th>Reading Comprehension</th>\n",
       "      <th>Math MMLU</th>\n",
       "      <th>BBH</th>\n",
       "      <th>AGI Eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MPT</td>\n",
       "      <td>7B</td>\n",
       "      <td>20.5</td>\n",
       "      <td>57.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>57.5</td>\n",
       "      <td>4.90</td>\n",
       "      <td>26.8</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MPT</td>\n",
       "      <td>30B</td>\n",
       "      <td>28.9</td>\n",
       "      <td>64.9</td>\n",
       "      <td>50.0</td>\n",
       "      <td>64.7</td>\n",
       "      <td>9.10</td>\n",
       "      <td>46.9</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Falcon</td>\n",
       "      <td>7B</td>\n",
       "      <td>5.6</td>\n",
       "      <td>56.1</td>\n",
       "      <td>42.8</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.60</td>\n",
       "      <td>26.2</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Falcon</td>\n",
       "      <td>40B</td>\n",
       "      <td>15.2</td>\n",
       "      <td>69.2</td>\n",
       "      <td>56.7</td>\n",
       "      <td>65.7</td>\n",
       "      <td>12.60</td>\n",
       "      <td>55.4</td>\n",
       "      <td>37.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Falcon</td>\n",
       "      <td>7B</td>\n",
       "      <td>14.1</td>\n",
       "      <td>60.8</td>\n",
       "      <td>46.2</td>\n",
       "      <td>58.5</td>\n",
       "      <td>6.95</td>\n",
       "      <td>35.1</td>\n",
       "      <td>30.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Llama 1</td>\n",
       "      <td>13B</td>\n",
       "      <td>18.9</td>\n",
       "      <td>66.1</td>\n",
       "      <td>52.6</td>\n",
       "      <td>62.3</td>\n",
       "      <td>10.90</td>\n",
       "      <td>46.9</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Llama 1</td>\n",
       "      <td>33B</td>\n",
       "      <td>26.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>58.4</td>\n",
       "      <td>67.6</td>\n",
       "      <td>21.40</td>\n",
       "      <td>57.8</td>\n",
       "      <td>39.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Llama 1</td>\n",
       "      <td>65B</td>\n",
       "      <td>30.7</td>\n",
       "      <td>70.7</td>\n",
       "      <td>60.5</td>\n",
       "      <td>68.6</td>\n",
       "      <td>30.80</td>\n",
       "      <td>63.4</td>\n",
       "      <td>43.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Llama 1</td>\n",
       "      <td>7B</td>\n",
       "      <td>16.8</td>\n",
       "      <td>63.9</td>\n",
       "      <td>48.9</td>\n",
       "      <td>61.3</td>\n",
       "      <td>14.60</td>\n",
       "      <td>45.3</td>\n",
       "      <td>32.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Llama 2</td>\n",
       "      <td>13B</td>\n",
       "      <td>24.5</td>\n",
       "      <td>66.9</td>\n",
       "      <td>55.4</td>\n",
       "      <td>65.8</td>\n",
       "      <td>28.70</td>\n",
       "      <td>54.8</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Llama 2</td>\n",
       "      <td>34B</td>\n",
       "      <td>27.8</td>\n",
       "      <td>69.9</td>\n",
       "      <td>58.7</td>\n",
       "      <td>68.0</td>\n",
       "      <td>24.20</td>\n",
       "      <td>62.6</td>\n",
       "      <td>44.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Llama 2</td>\n",
       "      <td>70B</td>\n",
       "      <td>37.5</td>\n",
       "      <td>71.9</td>\n",
       "      <td>63.6</td>\n",
       "      <td>69.4</td>\n",
       "      <td>35.20</td>\n",
       "      <td>68.9</td>\n",
       "      <td>51.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model Size  Code  Commonsense Reasoning  World Knowledge  \\\n",
       "0       MPT   7B  20.5                   57.4             41.0   \n",
       "1       MPT  30B  28.9                   64.9             50.0   \n",
       "2    Falcon   7B   5.6                   56.1             42.8   \n",
       "3    Falcon  40B  15.2                   69.2             56.7   \n",
       "4    Falcon   7B  14.1                   60.8             46.2   \n",
       "5   Llama 1  13B  18.9                   66.1             52.6   \n",
       "6   Llama 1  33B  26.0                   70.0             58.4   \n",
       "7   Llama 1  65B  30.7                   70.7             60.5   \n",
       "8   Llama 1   7B  16.8                   63.9             48.9   \n",
       "9   Llama 2  13B  24.5                   66.9             55.4   \n",
       "10  Llama 2  34B  27.8                   69.9             58.7   \n",
       "11  Llama 2  70B  37.5                   71.9             63.6   \n",
       "\n",
       "    Reading Comprehension  Math MMLU   BBH  AGI Eval  \n",
       "0                    57.5       4.90  26.8      31.0  \n",
       "1                    64.7       9.10  46.9      38.0  \n",
       "2                    36.0       4.60  26.2      28.0  \n",
       "3                    65.7      12.60  55.4      37.1  \n",
       "4                    58.5       6.95  35.1      30.3  \n",
       "5                    62.3      10.90  46.9      37.0  \n",
       "6                    67.6      21.40  57.8      39.8  \n",
       "7                    68.6      30.80  63.4      43.5  \n",
       "8                    61.3      14.60  45.3      32.6  \n",
       "9                    65.8      28.70  54.8      39.4  \n",
       "10                   68.0      24.20  62.6      44.1  \n",
       "11                   69.4      35.20  68.9      51.2  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "table_dfs = []\n",
    "for node in nodes:\n",
    "    if \"table\" in node.id_:\n",
    "        if \"table_df\" in node.metadata:\n",
    "            table_dfs.append(\n",
    "                pd.DataFrame.from_dict(\n",
    "                    ast.literal_eval(node.metadata[\"table_df\"])\n",
    "                )\n",
    "            )\n",
    "            # table_dfs.append(node.metadata[\"table_df\"])\n",
    "print(len(table_dfs))\n",
    "table_dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ffd3c",
   "metadata": {},
   "source": [
    "## Create Pandas Query Engines\n",
    "\n",
    "We create a pandas query engine over each structured table.\n",
    "\n",
    "These can be executed on their own to answer queries about each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.9\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import pandas as pd\n",
    "from llama_index.query_engine import PandasQueryEngine\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index.query_engine import PandasQueryEngine, RetrieverQueryEngine\n",
    "from llama_index.retrievers import RecursiveRetriever\n",
    "from llama_index.schema import IndexNode\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "# define query engines over these tables\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "df_query_engines = [\n",
    "    PandasQueryEngine(table_df, service_context=service_context)\n",
    "    for table_df in table_dfs\n",
    "]\n",
    "response_1 = df_query_engines[0].query(\n",
    "    \"What is MPT 30b performance for common sense reasoning?\"\n",
    ")\n",
    "print(response_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-vs8PXMh0-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
